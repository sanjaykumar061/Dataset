{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78198c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1/2] Creating global hash bin: 100%|██████████| 2/2 [19:01<00:00, 570.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique paragraphs across all shards: 39326958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2/2] Deduplicating across shards: 100%|██████████| 2/2 [19:16<00:00, 578.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Deduplication across shards complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "import unicodedata\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "INPUT_DIR = \"/home/blu-bridge004/Desktop/pytorch/preprocessed06_10\"           # Folder with input .jsonl files (1 doc per line)\n",
    "OUTPUT_DIR = \"/home/blu-bridge004/Desktop/pytorch/test2deduplicatedshards06_10\"     # Folder to save deduplicated output\n",
    "HASH_STORE_DIR = \"./hash_bins\"            # Where all paragraph hashes are stored\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(HASH_STORE_DIR, exist_ok=True)\n",
    "\n",
    "# === TEXT NORMALIZATION FUNCTION ===\n",
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\d\", \"0\", text)\n",
    "    text = ''.join(c for c in unicodedata.normalize(\"NFKD\", text)\n",
    "                  if not unicodedata.combining(c) and not unicodedata.category(c).startswith('P'))\n",
    "    return text.strip()\n",
    "\n",
    "# === HASHING FUNCTION (first 64 bits of SHA1) ===\n",
    "def paragraph_hash(paragraph):\n",
    "    norm = normalize_text(paragraph)\n",
    "    sha1 = hashlib.sha1(norm.encode(\"utf-8\")).hexdigest()\n",
    "    return sha1[:8]  # 64 bits = 16 hex chars\n",
    "\n",
    "# === STEP 1: CREATE A SINGLE GLOBAL HASH BIN WITH ALL SHARDS ===\n",
    "def create_global_hash_bin():\n",
    "    all_hashes = set()\n",
    "    \n",
    "    # Process each shard and add paragraph hashes to global set\n",
    "    for fname in tqdm(sorted(os.listdir(INPUT_DIR)), desc=\"[1/2] Creating global hash bin\"):\n",
    "        if not fname.endswith(\".jsonl\"): continue\n",
    "\n",
    "        with open(os.path.join(INPUT_DIR, fname), \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                doc = json.loads(line)\n",
    "                text = doc.get(\"text\", \"\")\n",
    "                paragraphs = [p.strip() for p in text.split(\"\\n\") if p.strip()]\n",
    "                for para in paragraphs:\n",
    "                    h = paragraph_hash(para)\n",
    "                    all_hashes.add(h)\n",
    "\n",
    "    # Save all unique hashes in a single global hash bin file\n",
    "    global_hash_bin_path = os.path.join(HASH_STORE_DIR, \"global_hashes.hashes\")\n",
    "    with open(global_hash_bin_path, \"w\") as out:\n",
    "        out.write(\"\\n\".join(sorted(all_hashes)))\n",
    "    \n",
    "    print(f\"Total unique paragraphs across all shards: {len(all_hashes)}\")\n",
    "\n",
    "# === STEP 2: DEDUPLICATE PARAGRAPHS ACROSS ALL SHARDS ===\n",
    "def deduplicate_across_shards():\n",
    "    # Load all hashes from the global hash bin\n",
    "    all_hashes = set()\n",
    "    global_hash_bin_path = os.path.join(HASH_STORE_DIR, \"global_hashes.hashes\")\n",
    "    \n",
    "    with open(global_hash_bin_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            all_hashes.add(line.strip())\n",
    "\n",
    "    # Process each shard for deduplication\n",
    "    for fname in tqdm(sorted(os.listdir(INPUT_DIR)), desc=\"[2/2] Deduplicating across shards\"):\n",
    "        if not fname.endswith(\".jsonl\"): continue\n",
    "\n",
    "        in_path = os.path.join(INPUT_DIR, fname)\n",
    "        out_path = os.path.join(OUTPUT_DIR, fname)\n",
    "\n",
    "        with open(in_path, \"r\", encoding=\"utf-8\") as in_f, \\\n",
    "             open(out_path, \"w\", encoding=\"utf-8\") as out_f:\n",
    "\n",
    "            for line in in_f:\n",
    "                doc = json.loads(line)\n",
    "                text = doc.get(\"text\", \"\")\n",
    "                new_paragraphs = []\n",
    "                for p in text.split(\"\\n\"):\n",
    "                    p = p.strip()\n",
    "                    if not p: continue\n",
    "                    h = paragraph_hash(p)\n",
    "                    if h in all_hashes:\n",
    "                        continue  # Skip duplicate paragraph\n",
    "                    new_paragraphs.append(p)\n",
    "                    all_hashes.add(h)  # Mark this paragraph hash as seen\n",
    "\n",
    "                if new_paragraphs:\n",
    "                    doc[\"text\"] = \"\\n\".join(new_paragraphs)\n",
    "                    out_f.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# === RUN FULL PIPELINE ===\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Create the global hash bin with unique paragraph hashes across all shards\n",
    "    create_global_hash_bin()\n",
    "    \n",
    "    # Step 2: Deduplicate paragraphs across all shards using the global hash bin\n",
    "    deduplicate_across_shards()\n",
    "    \n",
    "    print(\"\\n✅ Deduplication across shards complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
